name: Update Binance Futures Availability Database

# Run daily at 3:00 AM UTC (1 hour after S3 Vision data becomes available at 2:00 AM)
# Also allow manual triggering for gap-filling or testing
on:
  schedule:
    - cron: '0 3 * * *'  # Daily at 3:00 AM UTC
  workflow_dispatch:
    inputs:
      update_mode:
        description: 'Update mode: daily (yesterday only) or backfill (custom range)'
        required: true
        default: 'daily'
        type: choice
        options:
          - daily
          - backfill
      start_date:
        description: 'Backfill start date (YYYY-MM-DD) - only used if mode=backfill'
        required: false
        type: string
      end_date:
        description: 'Backfill end date (YYYY-MM-DD) - only used if mode=backfill'
        required: false
        type: string
      lookback_days:
        description: 'Lookback window (days) for daily mode - ADR-0011 testing (1=baseline, 7=incremental, 20=production)'
        required: false
        default: '1'
        type: string

permissions:
  contents: write  # For creating releases and pushing database
  pull-requests: write  # For creating PR comments (optional notifications)

jobs:
  update-database:
    runs-on: ubuntu-latest

    env:
      DB_PATH: ${{ github.workspace }}/.cache/binance-futures/availability.duckdb
      PYTHON_VERSION: '3.12'
      # ADR-0011: 20-Day Lookback for Data Reliability
      # workflow_dispatch: Use input parameter (testing: 1/7/20)
      # schedule (cron): Use default '1' (Phase 1: safe deployment)
      # Phase 3 (production): Change default to '20' for scheduled runs
      LOOKBACK_DAYS: ${{ github.event.inputs.lookback_days || '1' }}

    steps:
      # ============================================================================
      # SETUP: Environment and Dependencies
      # ============================================================================

      - name: Checkout repository
        uses: actions/checkout@v6  # ADR-0018: Upgrade to v6 LTS
        with:
          fetch-depth: 0  # Full history for git operations

      - name: Install uv (Python package manager)
        uses: astral-sh/setup-uv@v7
        with:
          version: "latest"
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v6  # ADR-0018: Upgrade to v6 LTS
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Verify AWS CLI (for bulk operations)
        run: aws --version

      - name: Install project dependencies
        run: |
          uv pip install --system -e ".[dev]"

      # ============================================================================
      # LINT: Code quality checks (ADR-0020: fail fast on quality issues)
      # ============================================================================

      - name: Run ruff linting
        run: |
          echo "Running ruff linter..."
          uv run ruff check src/ tests/ scripts/
          echo "âœ“ Linting passed"

      # ============================================================================
      # DISCOVER: Update symbols.json from S3 Vision (daily discovery)
      # ============================================================================

      - name: Discover symbols from S3
        id: discovery
        run: |
          echo "Discovering futures symbols from Binance Vision S3..."
          uv run python scripts/operations/discover_symbols.py

          if [ $? -ne 0 ]; then
            echo "âŒ Symbol discovery failed (strict mode per ADR-0010)"
            exit 1
          fi

          # Check if symbols.json changed
          if git diff --quiet src/binance_futures_availability/data/symbols.json; then
            echo "No symbol changes detected"
            echo "symbols_changed=false" >> $GITHUB_OUTPUT
          else
            echo "Symbol changes detected:"
            git diff --stat src/binance_futures_availability/data/symbols.json
            echo "symbols_changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Commit updated symbols
        if: steps.discovery.outputs.symbols_changed == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add src/binance_futures_availability/data/symbols.json
          git commit -m "chore(symbols): auto-update from S3 discovery

          Discovered new symbols via daily S3 XML API enumeration.

          [skip ci]"
          git push

      # ============================================================================
      # RESTORE: Download existing database from latest release
      # ============================================================================

      - name: Download existing database
        id: download_db
        run: |
          # Create cache directory
          mkdir -p $(dirname "$DB_PATH")

          # Try to download latest release asset
          gh release download latest \
            --pattern "availability.duckdb" \
            --output "$DB_PATH" \
            --clobber \
            2>/dev/null || echo "No existing database found, starting fresh"

          # Check if database exists and show info
          if [ -f "$DB_PATH" ]; then
            echo "Database found: $(ls -lh "$DB_PATH" | awk '{print $5}')"
            echo "db_exists=true" >> $GITHUB_OUTPUT

            # Query database stats
            uv run python .github/scripts/check_database_stats.py
          else
            echo "No existing database, will create new one"
            echo "db_exists=false" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # ============================================================================
      # GAP DETECTION: Identify new symbols needing historical backfill (ADR-0012)
      # ============================================================================

      - name: Detect symbol gaps
        id: detect_gaps
        if: steps.discovery.outputs.symbols_changed == 'true' && steps.download_db.outputs.db_exists == 'true'
        run: |
          echo "Checking for new symbols needing historical backfill..."

          # Run gap detection script (outputs JSON array of new symbols)
          NEW_SYMBOLS=$(uv run python scripts/operations/detect_symbol_gaps.py)

          echo "Gap detection output: $NEW_SYMBOLS"
          echo "new_symbols=$NEW_SYMBOLS" >> $GITHUB_OUTPUT

          # Check if JSON array is non-empty (has gaps)
          if [ "$NEW_SYMBOLS" != "[]" ] && [ -n "$NEW_SYMBOLS" ]; then
            echo "gaps_detected=true" >> $GITHUB_OUTPUT
            echo "âœ… Gaps detected: $NEW_SYMBOLS"
          else
            echo "gaps_detected=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ No gaps detected (all symbols already in database)"
          fi

      # ============================================================================
      # AUTO-BACKFILL: Backfill new symbols with historical data (ADR-0012)
      # ============================================================================

      - name: Auto-backfill new symbols
        id: auto_backfill
        if: steps.detect_gaps.outputs.gaps_detected == 'true'
        run: |
          NEW_SYMBOLS="${{ steps.detect_gaps.outputs.new_symbols }}"

          # Parse JSON array to comma-separated string
          # ["SYM1","SYM2"] â†’ SYM1,SYM2
          SYMBOL_LIST=$(echo "$NEW_SYMBOLS" | jq -r '.[] | @text' | tr '\n' ',' | sed 's/,$//')

          echo "ðŸ”„ Auto-backfilling new symbols: $SYMBOL_LIST"
          echo "Starting historical backfill from 2019-09-25 to yesterday..."

          # Run targeted backfill for new symbols only
          uv run python scripts/operations/backfill.py --symbols "$SYMBOL_LIST"

          if [ $? -eq 0 ]; then
            echo "backfill_success=true" >> $GITHUB_OUTPUT
            echo "âœ… Auto-backfill completed successfully"
          else
            echo "backfill_success=false" >> $GITHUB_OUTPUT
            echo "âŒ Auto-backfill failed"
            exit 1
          fi

      # ============================================================================
      # UPDATE: Run daily update or backfill based on trigger
      # ============================================================================

      - name: Run daily update (scheduled or manual daily mode)
        if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && inputs.update_mode == 'daily')
        run: |
          echo "Running daily update for yesterday's data..."

          # Run update for yesterday
          uv run python .github/scripts/run_daily_update.py

      - name: Run backfill (manual backfill mode)
        if: github.event_name == 'workflow_dispatch' && inputs.update_mode == 'backfill'
        run: |
          echo "Running backfill mode..."

          START_DATE="${{ inputs.start_date }}"
          END_DATE="${{ inputs.end_date }}"

          # Build command with optional date arguments
          CMD="uv run python scripts/operations/backfill.py"
          [ -n "$START_DATE" ] && CMD="$CMD --start-date $START_DATE"
          [ -n "$END_DATE" ] && CMD="$CMD --end-date $END_DATE"

          echo "Executing: $CMD"
          $CMD

      # ============================================================================
      # CHECK: Verify database exists after update/backfill
      # ============================================================================

      - name: Check if database exists
        id: check_db
        run: |
          if [ -f "$DB_PATH" ]; then
            echo "âœ… Database exists: $(ls -lh "$DB_PATH" | awk '{print $5}')"
            echo "db_ready=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ Database not found at $DB_PATH"
            echo "db_ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      # ============================================================================
      # VALIDATE: Run all validation checks
      # ============================================================================

      - name: Run validation checks
        id: validate
        if: steps.check_db.outputs.db_ready == 'true'
        run: |
          echo "Running database validation..."
          uv run python scripts/operations/validate.py --verbose

          # Capture validation results
          if [ $? -eq 0 ]; then
            echo "validation_passed=true" >> $GITHUB_OUTPUT
            echo "âœ… All validation checks passed"
          else
            echo "validation_passed=false" >> $GITHUB_OUTPUT
            echo "âŒ Validation checks failed"
            exit 1
          fi

      - name: Generate database statistics
        id: stats
        if: steps.check_db.outputs.db_ready == 'true'
        run: |
          echo "Generating database statistics..."

          # Query comprehensive stats
          uv run python .github/scripts/generate_stats.py

          # Read stats for GitHub output
          LATEST_DATE=$(cat stats.json | jq -r '.latest_date')
          TOTAL_RECORDS=$(cat stats.json | jq -r '.total_records')
          AVAILABILITY_PCT=$(cat stats.json | jq -r '.availability_pct')

          echo "latest_date=$LATEST_DATE" >> $GITHUB_OUTPUT
          echo "total_records=$TOTAL_RECORDS" >> $GITHUB_OUTPUT
          echo "availability_pct=$AVAILABILITY_PCT" >> $GITHUB_OUTPUT

      # ============================================================================
      # RANKINGS: Generate volume rankings time-series archive (ADR-0013)
      # ============================================================================

      - name: Download existing rankings archive
        id: download_rankings
        if: steps.check_db.outputs.db_ready == 'true'
        run: |
          RANKINGS_FILE="${{ github.workspace }}/.cache/binance-futures/volume-rankings-timeseries.parquet"

          echo "Attempting to download existing rankings archive..."

          # Try to download from latest release (may not exist on first run)
          if gh release download latest \
            --pattern "volume-rankings-timeseries.parquet" \
            --output "$RANKINGS_FILE" \
            --clobber \
            2>/dev/null; then

            echo "âœ… Existing rankings file found: $(ls -lh "$RANKINGS_FILE" | awk '{print $5}')"
            echo "rankings_exists=true" >> $GITHUB_OUTPUT
          else
            echo "â„¹ï¸ No existing rankings file, will generate from scratch"
            echo "rankings_exists=false" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Generate volume rankings
        id: generate_rankings
        if: steps.check_db.outputs.db_ready == 'true'
        continue-on-error: true  # Non-blocking: don't fail workflow if rankings generation fails
        run: |
          RANKINGS_FILE="${{ github.workspace }}/.cache/binance-futures/volume-rankings-timeseries.parquet"

          echo "Generating volume rankings time-series archive..."

          # Build command with optional existing file
          CMD="uv run python .github/scripts/generate_volume_rankings.py \
            --db-path \"$DB_PATH\" \
            --output \"$RANKINGS_FILE\""

          # Add existing file parameter if available (incremental append)
          if [ -f "$RANKINGS_FILE" ]; then
            CMD="$CMD --existing-file \"$RANKINGS_FILE\""
            echo "Mode: INCREMENTAL APPEND"
          else
            echo "Mode: FULL HISTORICAL GENERATION"
          fi

          # Execute generation script
          eval $CMD

          if [ $? -eq 0 ]; then
            echo "rankings_generated=true" >> $GITHUB_OUTPUT
            echo "âœ… Rankings generation completed successfully"

            # Extract summary stats from Parquet
            ROWS=$(uv run python -c "import pyarrow.parquet as pq; print(len(pq.read_table('$RANKINGS_FILE')))")
            SIZE_MB=$(du -m "$RANKINGS_FILE" | awk '{print $1}')

            echo "rankings_rows=$ROWS" >> $GITHUB_OUTPUT
            echo "rankings_size_mb=$SIZE_MB" >> $GITHUB_OUTPUT

            echo "ðŸ“Š Rankings stats: $ROWS rows, ${SIZE_MB}MB"
          else
            echo "rankings_generated=false" >> $GITHUB_OUTPUT
            echo "âŒ Rankings generation failed (non-blocking)"
            exit 1  # Fail step but continue workflow (continue-on-error: true)
          fi

      # ============================================================================
      # TEST: Run pytest to ensure code quality
      # ============================================================================

      - name: Run tests (excluding integration tests)
        run: |
          echo "Running unit tests..."
          uv run pytest -m "not integration" --cov-report=term

      # ============================================================================
      # PUBLISH: Create release and upload database
      # ============================================================================

      - name: Compress database for distribution
        if: steps.check_db.outputs.db_ready == 'true'
        run: |
          echo "Compressing database at: $DB_PATH"
          gzip -c "$DB_PATH" > "${DB_PATH}.gz"
          ls -lh "$DB_PATH" "${DB_PATH}.gz"
          echo "Compressed database ready: ${DB_PATH}.gz"

      - name: Generate release notes
        id: release_notes
        if: steps.check_db.outputs.db_ready == 'true'
        env:
          LATEST_DATE: ${{ steps.stats.outputs.latest_date }}
          TOTAL_RECORDS: ${{ steps.stats.outputs.total_records }}
          AVAILABILITY_PCT: ${{ steps.stats.outputs.availability_pct }}
          RANKINGS_GENERATED: ${{ steps.generate_rankings.outputs.rankings_generated }}
          RANKINGS_ROWS: ${{ steps.generate_rankings.outputs.rankings_rows }}
          RANKINGS_SIZE_MB: ${{ steps.generate_rankings.outputs.rankings_size_mb }}
          TRIGGER: ${{ github.event_name }}
          UPDATE_MODE: ${{ github.event_name == 'schedule' && 'daily' || inputs.update_mode }}
          VALIDATION_STATUS: ${{ steps.validate.outputs.validation_passed == 'true' && 'Passed' || 'Failed' }}
          REPO: ${{ github.repository }}
        run: |
          cat > release_notes.md << EOF
          ## Database Update - $(date -u +"%Y-%m-%d %H:%M UTC")

          ### Statistics
          - **Latest Date**: $LATEST_DATE
          - **Total Records**: $TOTAL_RECORDS
          - **Availability**: $AVAILABILITY_PCT%

          ### Update Details
          - **Trigger**: $TRIGGER
          - **Mode**: $UPDATE_MODE
          - **Validation**: $VALIDATION_STATUS

          ### Files
          - \`availability.duckdb\` - Uncompressed database (50-150 MB)
          - \`availability.duckdb.gz\` - Compressed database (recommended for download)
          - \`volume-rankings-timeseries.parquet\` - Volume rankings archive (ADR-0013)

          ### Volume Rankings Archive
          $(if [ "$RANKINGS_GENERATED" = "true" ]; then
            echo "- **Status**: âœ… Generated successfully"
            echo "- **Rows**: ${RANKINGS_ROWS:-N/A}"
            echo "- **Size**: ${RANKINGS_SIZE_MB:-N/A} MB"
            echo "- **Format**: Parquet (columnar, SNAPPY compressed)"
            echo "- **Schema**: 13 columns (date, symbol, rank, volume metrics, 4 rank changes)"
          else
            echo "- **Status**: âš ï¸ Generation failed (database still published)"
          fi)

          ### Usage
          \`\`\`bash
          # Download database
          wget https://github.com/$REPO/releases/download/latest/availability.duckdb.gz
          gunzip availability.duckdb.gz

          # Download rankings archive
          wget https://github.com/$REPO/releases/download/latest/volume-rankings-timeseries.parquet

          # Query database with Python
          import duckdb
          conn = duckdb.connect('availability.duckdb', read_only=True)
          result = conn.execute('SELECT * FROM daily_availability LIMIT 10').fetchall()

          # Query rankings with DuckDB
          import duckdb
          conn = duckdb.connect()
          rankings = conn.execute('''
              SELECT * FROM read_parquet('volume-rankings-timeseries.parquet')
              WHERE date = '2025-11-16'
              ORDER BY rank
              LIMIT 10
          ''').fetchall()
          \`\`\`
          EOF

          # Output for GitHub summary
          echo "release_notes_file=release_notes.md" >> $GITHUB_OUTPUT

      - name: Create/Update release
        if: steps.check_db.outputs.db_ready == 'true'
        uses: softprops/action-gh-release@v2
        with:
          tag_name: latest
          name: Latest Database Snapshot
          body_path: release_notes.md
          files: |
            ${{ github.workspace }}/.cache/binance-futures/availability.duckdb
            ${{ github.workspace }}/.cache/binance-futures/availability.duckdb.gz
            ${{ github.workspace }}/.cache/binance-futures/volume-rankings-timeseries.parquet
          draft: false
          prerelease: false
          make_latest: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # ============================================================================
      # NOTIFY: Post summary to GitHub Actions summary
      # ============================================================================

      - name: Create GitHub Actions summary
        if: always()
        env:
          VALIDATION_STATUS: ${{ steps.validate.outputs.validation_passed == 'true' && 'Passed' || 'Failed' }}
          LATEST_DATE: ${{ steps.stats.outputs.latest_date }}
          TOTAL_RECORDS: ${{ steps.stats.outputs.total_records }}
          AVAILABILITY_PCT: ${{ steps.stats.outputs.availability_pct }}
          RANKINGS_GENERATED: ${{ steps.generate_rankings.outputs.rankings_generated }}
          RANKINGS_ROWS: ${{ steps.generate_rankings.outputs.rankings_rows }}
          RANKINGS_SIZE_MB: ${{ steps.generate_rankings.outputs.rankings_size_mb }}
          TRIGGER: ${{ github.event_name }}
          RUN_ID: ${{ github.run_id }}
          REPO: ${{ github.repository }}
        run: |
          cat >> $GITHUB_STEP_SUMMARY << EOF
          # Database Update Summary

          ## Status
          - **Validation**: $VALIDATION_STATUS
          - **Latest Date**: $LATEST_DATE
          - **Total Records**: $TOTAL_RECORDS
          - **Availability**: $AVAILABILITY_PCT%

          ## Volume Rankings (ADR-0013)
          $(if [ "$RANKINGS_GENERATED" = "true" ]; then
            echo "- **Status**: âœ… Generated"
            echo "- **Rows**: ${RANKINGS_ROWS:-N/A}"
            echo "- **Size**: ${RANKINGS_SIZE_MB:-N/A} MB"
          else
            echo "- **Status**: âš ï¸ Failed (non-blocking)"
          fi)

          ## Workflow Details
          - **Trigger**: $TRIGGER
          - **Run ID**: $RUN_ID
          - **Timestamp**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Next Steps
          - Database published to [latest release](https://github.com/$REPO/releases/tag/latest)
          - Download database: \`wget https://github.com/$REPO/releases/download/latest/availability.duckdb.gz\`
          - Download rankings: \`wget https://github.com/$REPO/releases/download/latest/volume-rankings-timeseries.parquet\`
          EOF

      # ============================================================================
      # CLEANUP: Remove temporary files
      # ============================================================================

      - name: Cleanup
        if: always()
        run: |
          rm -f stats.json release_notes.md awscliv2.zip
          rm -rf aws/
